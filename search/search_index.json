{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"DeepPhonemizer DeepPhonemizer is a library for grapheme to phoneme conversion based on Transformer models. It is intended to be used in text-to-speech production systems with high accuracy and efficiency. You can choose between a forward Transformer model (trained with CTC) and its autoregressive counterpart. The former is faster and more stable while the latter is slightly more accurate. The main advantages of this repo are: Easy-to-use API for training and inference. Multilingual: You can train a single model on several languages. Accuracy: Phoneme and word error rates are comparable to state-of-art. Speed: The repo is highly optimized for fast inference by using dictionaries and batching. Check out the inference and training tutorials on Colab! Read the documentation at: https://as-ideas.github.io/DeepPhonemizer/ Installation pip install deep-phonemizer Quickstart Download the pretrained model: en_us_cmudict_ipa_forward from dp.phonemizer import Phonemizer phonemizer = Phonemizer.from_checkpoint ( 'en_us_cmudict_ipa.pt' ) phonemizer ( 'Phonemizing an English text is imposimpable!' , lang = 'en_us' ) 'fo\u028an\u026ama\u026az\u026a\u014b \u00e6n \u026a\u014bgl\u026a\u0283 t\u025bkst \u026az \u026amp\u0259z\u026amp\u0259b\u0259l!' Training You can easily train your own autoregressive or forward transformer model. All necessary parameters are set in a config.yaml, which you can find under: dp/configs/forward_config.yaml dp/configs/autoreg_config.yaml for the forward and autoregressive transformer model, respectively. Pepare data in a tuple-format and use the preprocess and train API: from dp.preprocess import preprocess from dp.train import train train_data = [( 'en_us' , 'young' , 'j\u028c\u014b' ), ( 'de' , 'ben\u00fctzten' , 'b\u0259n\u028ft\u0361stn\u0329' ), ( 'de' , 'gew\u00fcrz' , '\u0261\u0259v\u028f\u0281t\u0361s' )] * 1000 val_data = [( 'en_us' , 'young' , 'j\u028c\u014b' ), ( 'de' , 'ben\u00fctzten' , 'b\u0259n\u028ft\u0361stn\u0329' )] * 100 preprocess ( config_file = 'config.yaml' , train_data = train_data , deduplicate_train_data = False ) train ( config_file = 'config.yaml' ) Model checkpoints will be stored in the checkpoints path that is provided by the config.yaml. Inference Load the phonemizer from a checkpoint and run a prediction. By default, the phonemizer stores a dictionary of word-phoneme mappings that is applied first, and it uses the Transformer model only to predict out-of-dictionary words. from dp.phonemizer import Phonemizer phonemizer = Phonemizer . from_checkpoint ( 'checkpoints/best_model.pt' ) phonemes = phonemizer ( 'Phonemizing an English text is imposimpable!' , lang = 'en_us' ) If you need more inference information, you can use following API: from dp.phonemizer import Phonemizer result = phonemizer . phonemise_list ([ 'Phonemizing an English text is imposimpable!' ], lang = 'en_us' ) for word , pred in result . predictions . items (): print ( f ' { word } { pred . phonemes } { pred . confidence } ' ) Pretrained Models Model Language Dataset Repo Version en_us_cmudict_ipa_forward en_us cmudict-ipa 0.0.10 en_us_cmudict_forward en_us cmudict 0.0.10 latin_ipa_forward en_uk, en_us, de, fr, es wikipron 0.0.10 Torchscript Export You can easily export the underlying transformer models with TorchScript: import torch from dp.phonemizer import Phonemizer phonemizer = Phonemizer . from_checkpoint ( 'checkpoints/best_model.pt' ) model = phonemizer . predictor . model phonemizer . predictor . model = torch . jit . script ( model ) phonemizer ( 'Running the torchscript model!' ) Maintainers Christian Sch\u00e4fer, github: cschaefer26 References Transformer based Grapheme-to-Phoneme Conversion GRAPHEME-TO-PHONEME CONVERSION USING LONG SHORT-TERM MEMORY RECURRENT NEURAL NETWORKS","title":"Home"},{"location":"index.html#deepphonemizer","text":"DeepPhonemizer is a library for grapheme to phoneme conversion based on Transformer models. It is intended to be used in text-to-speech production systems with high accuracy and efficiency. You can choose between a forward Transformer model (trained with CTC) and its autoregressive counterpart. The former is faster and more stable while the latter is slightly more accurate. The main advantages of this repo are: Easy-to-use API for training and inference. Multilingual: You can train a single model on several languages. Accuracy: Phoneme and word error rates are comparable to state-of-art. Speed: The repo is highly optimized for fast inference by using dictionaries and batching. Check out the inference and training tutorials on Colab! Read the documentation at: https://as-ideas.github.io/DeepPhonemizer/","title":"DeepPhonemizer"},{"location":"index.html#installation","text":"pip install deep-phonemizer","title":"Installation"},{"location":"index.html#quickstart","text":"Download the pretrained model: en_us_cmudict_ipa_forward from dp.phonemizer import Phonemizer phonemizer = Phonemizer.from_checkpoint ( 'en_us_cmudict_ipa.pt' ) phonemizer ( 'Phonemizing an English text is imposimpable!' , lang = 'en_us' ) 'fo\u028an\u026ama\u026az\u026a\u014b \u00e6n \u026a\u014bgl\u026a\u0283 t\u025bkst \u026az \u026amp\u0259z\u026amp\u0259b\u0259l!'","title":"Quickstart"},{"location":"index.html#training","text":"You can easily train your own autoregressive or forward transformer model. All necessary parameters are set in a config.yaml, which you can find under: dp/configs/forward_config.yaml dp/configs/autoreg_config.yaml for the forward and autoregressive transformer model, respectively. Pepare data in a tuple-format and use the preprocess and train API: from dp.preprocess import preprocess from dp.train import train train_data = [( 'en_us' , 'young' , 'j\u028c\u014b' ), ( 'de' , 'ben\u00fctzten' , 'b\u0259n\u028ft\u0361stn\u0329' ), ( 'de' , 'gew\u00fcrz' , '\u0261\u0259v\u028f\u0281t\u0361s' )] * 1000 val_data = [( 'en_us' , 'young' , 'j\u028c\u014b' ), ( 'de' , 'ben\u00fctzten' , 'b\u0259n\u028ft\u0361stn\u0329' )] * 100 preprocess ( config_file = 'config.yaml' , train_data = train_data , deduplicate_train_data = False ) train ( config_file = 'config.yaml' ) Model checkpoints will be stored in the checkpoints path that is provided by the config.yaml.","title":"Training"},{"location":"index.html#inference","text":"Load the phonemizer from a checkpoint and run a prediction. By default, the phonemizer stores a dictionary of word-phoneme mappings that is applied first, and it uses the Transformer model only to predict out-of-dictionary words. from dp.phonemizer import Phonemizer phonemizer = Phonemizer . from_checkpoint ( 'checkpoints/best_model.pt' ) phonemes = phonemizer ( 'Phonemizing an English text is imposimpable!' , lang = 'en_us' ) If you need more inference information, you can use following API: from dp.phonemizer import Phonemizer result = phonemizer . phonemise_list ([ 'Phonemizing an English text is imposimpable!' ], lang = 'en_us' ) for word , pred in result . predictions . items (): print ( f ' { word } { pred . phonemes } { pred . confidence } ' )","title":"Inference"},{"location":"index.html#pretrained-models","text":"Model Language Dataset Repo Version en_us_cmudict_ipa_forward en_us cmudict-ipa 0.0.10 en_us_cmudict_forward en_us cmudict 0.0.10 latin_ipa_forward en_uk, en_us, de, fr, es wikipron 0.0.10","title":"Pretrained Models"},{"location":"index.html#torchscript-export","text":"You can easily export the underlying transformer models with TorchScript: import torch from dp.phonemizer import Phonemizer phonemizer = Phonemizer . from_checkpoint ( 'checkpoints/best_model.pt' ) model = phonemizer . predictor . model phonemizer . predictor . model = torch . jit . script ( model ) phonemizer ( 'Running the torchscript model!' )","title":"Torchscript Export"},{"location":"index.html#maintainers","text":"Christian Sch\u00e4fer, github: cschaefer26","title":"Maintainers"},{"location":"index.html#references","text":"Transformer based Grapheme-to-Phoneme Conversion GRAPHEME-TO-PHONEME CONVERSION USING LONG SHORT-TERM MEMORY RECURRENT NEURAL NETWORKS","title":"References"},{"location":"CONTRIBUTING.html","text":"Contribution Guide We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our master branch of the original DeepPhonemizer repo. Documentation Make sure any new function or class you introduce has proper docstrings. Testing We use unittest for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contributing"},{"location":"CONTRIBUTING.html#contribution-guide","text":"We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING.html#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING.html#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs:"},{"location":"CONTRIBUTING.html#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING.html#pull-requests-pr","text":"Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our master branch of the original DeepPhonemizer repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING.html#documentation","text":"Make sure any new function or class you introduce has proper docstrings.","title":"Documentation"},{"location":"CONTRIBUTING.html#testing","text":"We use unittest for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING.html#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE.html","text":"MIT License Copyright (c) 2021 Axel Springer News Media & Tech GmbH & Co. KG - Ideas Engineering Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"LICENSE"},{"location":"phonemizer.html","text":"class Phonemizer __init__ def __init__ ( predictor , lang_phoneme_dict ) Initializes a phonemizer with a ready predictor. Args predictor (Predictor) : Predictor object carrying the trained transformer model. lang_phoneme_dict (Dict[str, Dict[str, str]], optional) : Word-phoneme dictionary for each language. __call__ def __call__ ( text , lang , punctuation , expand_acronyms , batch_size ) Phonemizes a single text or list of texts. Args text (str) : Text to phonemize as single string or list of strings. lang (str) : Language used for phonemization. punctuation (str) : Punctuation symbols by which the texts are split. expand_acronyms (bool) : Whether to expand an acronym, e.g. DIY -> D-I-Y. batch_size (int) : Batch size of model to speed up inference. Returns Union[str, List[str]] : Phonemized text as string, or list of strings, respectively. phonemise_list def phonemise_list ( texts , lang , punctuation , expand_acronyms , batch_size ) Phonemizes a list of texts and returns tokenized texts, phonemes and word predictions with probabilities. Args texts (List[str]) : List texts to phonemize. lang (str) : Language used for phonemization. punctuation (str) : Punctuation symbols by which the texts are split. (Default value = DEFAULT_PUNCTUATION) expand_acronyms (bool) : Whether to expand an acronym, e.g. DIY -> D-I-Y. (Default value = True) batch_size (int) : Batch size of model to speed up inference. (Default value = 8) Returns PhonemizerResult : Object containing original texts, phonemes, split texts, split phonemes, and predictions. from_checkpoint def from_checkpoint ( cls , checkpoint_path , device , lang_phoneme_dict ) Initializes a Phonemizer object from a model checkpoint (.pt file). Args checkpoint_path (str) : Path to the .pt checkpoint file. device (str) : Device to send the model to ('cpu' or 'cuda'). (Default value = 'cpu') lang_phoneme_dict (Dict[str, Dict[str, str]], optional) : Word-phoneme dictionary for each language. Returns Phonemizer : Phonemizer object carrying the loaded model and, optionally, a phoneme dictionary.","title":"Phonemizer"},{"location":"phonemizer.html#class-phonemizer","text":"","title":"class Phonemizer"},{"location":"phonemizer.html#__init__","text":"def __init__ ( predictor , lang_phoneme_dict ) Initializes a phonemizer with a ready predictor.","title":"__init__"},{"location":"phonemizer.html#args","text":"predictor (Predictor) : Predictor object carrying the trained transformer model. lang_phoneme_dict (Dict[str, Dict[str, str]], optional) : Word-phoneme dictionary for each language.","title":"Args"},{"location":"phonemizer.html#__call__","text":"def __call__ ( text , lang , punctuation , expand_acronyms , batch_size ) Phonemizes a single text or list of texts.","title":"__call__"},{"location":"phonemizer.html#args_1","text":"text (str) : Text to phonemize as single string or list of strings. lang (str) : Language used for phonemization. punctuation (str) : Punctuation symbols by which the texts are split. expand_acronyms (bool) : Whether to expand an acronym, e.g. DIY -> D-I-Y. batch_size (int) : Batch size of model to speed up inference.","title":"Args"},{"location":"phonemizer.html#returns","text":"Union[str, List[str]] : Phonemized text as string, or list of strings, respectively.","title":"Returns"},{"location":"phonemizer.html#phonemise_list","text":"def phonemise_list ( texts , lang , punctuation , expand_acronyms , batch_size ) Phonemizes a list of texts and returns tokenized texts, phonemes and word predictions with probabilities.","title":"phonemise_list"},{"location":"phonemizer.html#args_2","text":"texts (List[str]) : List texts to phonemize. lang (str) : Language used for phonemization. punctuation (str) : Punctuation symbols by which the texts are split. (Default value = DEFAULT_PUNCTUATION) expand_acronyms (bool) : Whether to expand an acronym, e.g. DIY -> D-I-Y. (Default value = True) batch_size (int) : Batch size of model to speed up inference. (Default value = 8)","title":"Args"},{"location":"phonemizer.html#returns_1","text":"PhonemizerResult : Object containing original texts, phonemes, split texts, split phonemes, and predictions.","title":"Returns"},{"location":"phonemizer.html#from_checkpoint","text":"def from_checkpoint ( cls , checkpoint_path , device , lang_phoneme_dict ) Initializes a Phonemizer object from a model checkpoint (.pt file).","title":"from_checkpoint"},{"location":"phonemizer.html#args_3","text":"checkpoint_path (str) : Path to the .pt checkpoint file. device (str) : Device to send the model to ('cpu' or 'cuda'). (Default value = 'cpu') lang_phoneme_dict (Dict[str, Dict[str, str]], optional) : Word-phoneme dictionary for each language.","title":"Args"},{"location":"phonemizer.html#returns_2","text":"Phonemizer : Phonemizer object carrying the loaded model and, optionally, a phoneme dictionary.","title":"Returns"},{"location":"preprocess.html","text":"preprocess def preprocess ( config_file , train_data , val_data , deduplicate_train_data ) Preprocesses a given dataset to enable model training. The preprocessing result is stored in a folder provied by the config. Args config_file (str) : Path to the config.yaml that provides all necessary hyperparameters. train_data (List[Tuple[str, Iterable[str], Iterable[str]]]) : Training data as a list of Tuples (language, grapheme sequence, phoneme sequence). val_data (List[Tuple[str, Iterable[str], Iterable[str]]], optional) : Validation data as a list of Tuples (language, grapheme sequence, phoneme sequence). deduplicate_train_data (bool) : Whether to deduplicate multiple occurences of the same word, the first is taken (Default value = True). Returns None : the preprocessing result is stored in a folder provided by the config.","title":"preprocess"},{"location":"preprocess.html#preprocess","text":"def preprocess ( config_file , train_data , val_data , deduplicate_train_data ) Preprocesses a given dataset to enable model training. The preprocessing result is stored in a folder provied by the config.","title":"preprocess"},{"location":"preprocess.html#args","text":"config_file (str) : Path to the config.yaml that provides all necessary hyperparameters. train_data (List[Tuple[str, Iterable[str], Iterable[str]]]) : Training data as a list of Tuples (language, grapheme sequence, phoneme sequence). val_data (List[Tuple[str, Iterable[str], Iterable[str]]], optional) : Validation data as a list of Tuples (language, grapheme sequence, phoneme sequence). deduplicate_train_data (bool) : Whether to deduplicate multiple occurences of the same word, the first is taken (Default value = True).","title":"Args"},{"location":"preprocess.html#returns","text":"None : the preprocessing result is stored in a folder provided by the config.","title":"Returns"},{"location":"result.html","text":"class Prediction Container for single word prediction result. __init__ def __init__ ( word , phonemes , phoneme_tokens , confidence , token_probs ) Initializes a Prediction object. Args word (str) : Original word to predict. phonemes (str) : Predicted phonemes (without special tokens). phoneme_tokens (List[str]) : Predicted phoneme tokens (including special tokens). confidence (float) : Total confidence of result. token_probs (List[float]) : Probability of each phoneme token. class PhonemizerResult Container for phonemizer output. __init__ def __init__ ( text , phonemes , split_text , split_phonemes , predictions ) Initializes a PhonemizerResult object. Args text (List[str]) : List of input texts. phonemes (List[str]) : List of output phonemes. split_text (List[List[str]]) : List of texts, where each text is split into words and special chars. split_phonemes (List[List[str]]) : List of phonemes corresponding to split_text. predictions (Dict[str, Prediction]) : Dictionary with entries word to Tuple (phoneme, probability).","title":"Result"},{"location":"result.html#class-prediction","text":"Container for single word prediction result.","title":"class Prediction"},{"location":"result.html#__init__","text":"def __init__ ( word , phonemes , phoneme_tokens , confidence , token_probs ) Initializes a Prediction object.","title":"__init__"},{"location":"result.html#args","text":"word (str) : Original word to predict. phonemes (str) : Predicted phonemes (without special tokens). phoneme_tokens (List[str]) : Predicted phoneme tokens (including special tokens). confidence (float) : Total confidence of result. token_probs (List[float]) : Probability of each phoneme token.","title":"Args"},{"location":"result.html#class-phonemizerresult","text":"Container for phonemizer output.","title":"class PhonemizerResult"},{"location":"result.html#__init___1","text":"def __init__ ( text , phonemes , split_text , split_phonemes , predictions ) Initializes a PhonemizerResult object.","title":"__init__"},{"location":"result.html#args_1","text":"text (List[str]) : List of input texts. phonemes (List[str]) : List of output phonemes. split_text (List[List[str]]) : List of texts, where each text is split into words and special chars. split_phonemes (List[List[str]]) : List of phonemes corresponding to split_text. predictions (Dict[str, Prediction]) : Dictionary with entries word to Tuple (phoneme, probability).","title":"Args"},{"location":"train.html","text":"train def train ( config_file , checkpoint_file ) Runs training of a transformer model. Args config_file (str) : Path to the config.yaml that stores all necessary parameters. checkpoint_file (str, optional) : Path to a model checkpoint to resume training for (e.g. latest_model.pt) Returns None : The model checkpoints are stored in a folder provided by the config.","title":"train"},{"location":"train.html#train","text":"def train ( config_file , checkpoint_file ) Runs training of a transformer model.","title":"train"},{"location":"train.html#args","text":"config_file (str) : Path to the config.yaml that stores all necessary parameters. checkpoint_file (str, optional) : Path to a model checkpoint to resume training for (e.g. latest_model.pt)","title":"Args"},{"location":"train.html#returns","text":"None : The model checkpoints are stored in a folder provided by the config.","title":"Returns"},{"location":"model/model.html","text":"create_model def create_model ( model_type , config ) Initializes a model from a config for a given model type. Args model_type (ModelType) : Type of model to be initialized. config (dict) : Configuration containing hyperparams. load_checkpoint def load_checkpoint ( checkpoint_path , device ) Initializes a model from a checkpoint (.pt file). Args checkpoint_path (str) : Path to checkpoint file (.pt). device (str) : Device to put the model to ('cpu' or 'cuda'). Returns class ModelType is_autoregressive def is_autoregressive () Returns: bool: Whether the model is autoregressive. class Model __init__ def __init__ () generate def generate ( batch ) Generates phonemes for a text batch Args batch (Dict[str, torch.Tensor]) : Dictionary containing 'text' (tokenized text tensor), 'text_len' (text length tensor), 'start_index' (phoneme start indices for AutoregressiveTransformer) Returns Tuple[torch.Tensor, torch.Tensor] : The predictions. The first element is a tensor (phoneme tokens) class ForwardTransformer __init__ def __init__ ( encoder_vocab_size , decoder_vocab_size , d_model , d_fft , layers , dropout , heads ) forward def forward ( batch ) Forward pass of the model on a data batch. Args batch (Dict[str, torch.Tensor]) : Input batch entry 'text' (text tensor). Returns Tensor : Predictions. generate def generate ( batch ) Inference pass on a batch of tokenized texts. Args batch (Dict[str, torch.Tensor]) : Input batch with entry 'text' (text tensor). Returns Tuple : The first element is a Tensor (phoneme tokens) and the second element is a tensor (phoneme token probabilities). from_config def from_config ( cls , config ) class AutoregressiveTransformer __init__ def __init__ ( encoder_vocab_size , decoder_vocab_size , end_index , d_model , d_fft , encoder_layers , decoder_layers , dropout , heads ) forward def forward ( batch ) Foward pass of the model on a data batch. Args batch (Dict[str, torch.Tensor]) : Input batch with entries 'text' (text tensor) and 'phonemes' (phoneme tensor for teacher forcing). Returns Tensor : Predictions. generate def generate ( batch , max_len ) Inference pass on a batch of tokenized texts. Args batch (Dict[str, torch.Tensor]) : Dictionary containing the input to the model with entries 'text' and 'start_index' max_len (int) : Max steps of the autoregressive inference loop. Returns Tuple : Predictions. The first element is a Tensor of phoneme tokens and the second element is a Tensor of phoneme token probabilities. from_config def from_config ( cls , config ) Initializes an autoregressive Transformer model from a config. Args: config (dict): Configuration containing the hyperparams. Returns AutoregressiveTransformer : Model object.","title":"Model"},{"location":"model/model.html#create_model","text":"def create_model ( model_type , config ) Initializes a model from a config for a given model type.","title":"create_model"},{"location":"model/model.html#args","text":"model_type (ModelType) : Type of model to be initialized. config (dict) : Configuration containing hyperparams.","title":"Args"},{"location":"model/model.html#load_checkpoint","text":"def load_checkpoint ( checkpoint_path , device ) Initializes a model from a checkpoint (.pt file).","title":"load_checkpoint"},{"location":"model/model.html#args_1","text":"checkpoint_path (str) : Path to checkpoint file (.pt). device (str) : Device to put the model to ('cpu' or 'cuda').","title":"Args"},{"location":"model/model.html#returns","text":"","title":"Returns"},{"location":"model/model.html#class-modeltype","text":"","title":"class ModelType"},{"location":"model/model.html#is_autoregressive","text":"def is_autoregressive () Returns: bool: Whether the model is autoregressive.","title":"is_autoregressive"},{"location":"model/model.html#class-model","text":"","title":"class Model"},{"location":"model/model.html#__init__","text":"def __init__ ()","title":"__init__"},{"location":"model/model.html#generate","text":"def generate ( batch ) Generates phonemes for a text batch","title":"generate"},{"location":"model/model.html#args_2","text":"batch (Dict[str, torch.Tensor]) : Dictionary containing 'text' (tokenized text tensor), 'text_len' (text length tensor), 'start_index' (phoneme start indices for AutoregressiveTransformer)","title":"Args"},{"location":"model/model.html#returns_1","text":"Tuple[torch.Tensor, torch.Tensor] : The predictions. The first element is a tensor (phoneme tokens)","title":"Returns"},{"location":"model/model.html#class-forwardtransformer","text":"","title":"class ForwardTransformer"},{"location":"model/model.html#__init___1","text":"def __init__ ( encoder_vocab_size , decoder_vocab_size , d_model , d_fft , layers , dropout , heads )","title":"__init__"},{"location":"model/model.html#forward","text":"def forward ( batch ) Forward pass of the model on a data batch.","title":"forward"},{"location":"model/model.html#args_3","text":"batch (Dict[str, torch.Tensor]) : Input batch entry 'text' (text tensor).","title":"Args"},{"location":"model/model.html#returns_2","text":"Tensor : Predictions.","title":"Returns"},{"location":"model/model.html#generate_1","text":"def generate ( batch ) Inference pass on a batch of tokenized texts.","title":"generate"},{"location":"model/model.html#args_4","text":"batch (Dict[str, torch.Tensor]) : Input batch with entry 'text' (text tensor).","title":"Args"},{"location":"model/model.html#returns_3","text":"Tuple : The first element is a Tensor (phoneme tokens) and the second element is a tensor (phoneme token probabilities).","title":"Returns"},{"location":"model/model.html#from_config","text":"def from_config ( cls , config )","title":"from_config"},{"location":"model/model.html#class-autoregressivetransformer","text":"","title":"class AutoregressiveTransformer"},{"location":"model/model.html#__init___2","text":"def __init__ ( encoder_vocab_size , decoder_vocab_size , end_index , d_model , d_fft , encoder_layers , decoder_layers , dropout , heads )","title":"__init__"},{"location":"model/model.html#forward_1","text":"def forward ( batch ) Foward pass of the model on a data batch.","title":"forward"},{"location":"model/model.html#args_5","text":"batch (Dict[str, torch.Tensor]) : Input batch with entries 'text' (text tensor) and 'phonemes' (phoneme tensor for teacher forcing).","title":"Args"},{"location":"model/model.html#returns_4","text":"Tensor : Predictions.","title":"Returns"},{"location":"model/model.html#generate_2","text":"def generate ( batch , max_len ) Inference pass on a batch of tokenized texts.","title":"generate"},{"location":"model/model.html#args_6","text":"batch (Dict[str, torch.Tensor]) : Dictionary containing the input to the model with entries 'text' and 'start_index' max_len (int) : Max steps of the autoregressive inference loop.","title":"Args"},{"location":"model/model.html#returns_5","text":"Tuple : Predictions. The first element is a Tensor of phoneme tokens and the second element is a Tensor of phoneme token probabilities.","title":"Returns"},{"location":"model/model.html#from_config_1","text":"def from_config ( cls , config ) Initializes an autoregressive Transformer model from a config. Args: config (dict): Configuration containing the hyperparams.","title":"from_config"},{"location":"model/model.html#returns_6","text":"AutoregressiveTransformer : Model object.","title":"Returns"},{"location":"model/predictor.html","text":"class Predictor Performs model predictions on a batch of inputs. __init__ def __init__ ( model , preprocessor ) Initializes a Predictor object with a trained transformer model a preprocessor. Args model (Model) : Trained transformer model. preprocessor (Preprocessor) : Preprocessor corresponding to the model configuration. __call__ def __call__ ( words , lang , batch_size ) Predicts phonemes for a list of words. Args words (list) : List of words to predict. lang (str) : Language of texts. batch_size (int) : Size of batch for model input to speed up inference. Returns List[Prediction] : A list of result objects containing (word, phonemes, phoneme_tokens, token_probs, confidence) from_checkpoint def from_checkpoint ( cls , checkpoint_path , device ) Initializes the predictor from a checkpoint (.pt file). Args checkpoint_path (str) : Path to the checkpoint file (.pt). device (str) : Device to load the model on ('cpu' or 'cuda'). (Default value = 'cpu'). Returns Predictor : Predictor object.","title":"Predictor"},{"location":"model/predictor.html#class-predictor","text":"Performs model predictions on a batch of inputs.","title":"class Predictor"},{"location":"model/predictor.html#__init__","text":"def __init__ ( model , preprocessor ) Initializes a Predictor object with a trained transformer model a preprocessor.","title":"__init__"},{"location":"model/predictor.html#args","text":"model (Model) : Trained transformer model. preprocessor (Preprocessor) : Preprocessor corresponding to the model configuration.","title":"Args"},{"location":"model/predictor.html#__call__","text":"def __call__ ( words , lang , batch_size ) Predicts phonemes for a list of words.","title":"__call__"},{"location":"model/predictor.html#args_1","text":"words (list) : List of words to predict. lang (str) : Language of texts. batch_size (int) : Size of batch for model input to speed up inference.","title":"Args"},{"location":"model/predictor.html#returns","text":"List[Prediction] : A list of result objects containing (word, phonemes, phoneme_tokens, token_probs, confidence)","title":"Returns"},{"location":"model/predictor.html#from_checkpoint","text":"def from_checkpoint ( cls , checkpoint_path , device ) Initializes the predictor from a checkpoint (.pt file).","title":"from_checkpoint"},{"location":"model/predictor.html#args_2","text":"checkpoint_path (str) : Path to the checkpoint file (.pt). device (str) : Device to load the model on ('cpu' or 'cuda'). (Default value = 'cpu').","title":"Args"},{"location":"model/predictor.html#returns_1","text":"Predictor : Predictor object.","title":"Returns"},{"location":"model/utils.html","text":"get_dedup_tokens def get_dedup_tokens ( logits_batch ) Converts a batch of logits into the batch most probable tokens and their probabilities. Args logits_batch (Tensor) : torch.Tensor Returns Tuple : Deduplicated tokens. The first element is a tensor (token indices) and the second element _generate_square_subsequent_mask def _generate_square_subsequent_mask ( sz ) _make_len_mask def _make_len_mask ( inp ) _get_len_util_stop def _get_len_util_stop ( sequence , end_index ) _trim_util_stop def _trim_util_stop ( sequence , end_index ) class PositionalEncoding __init__ def __init__ ( d_model , dropout , max_len ) Initializes positional encoding. Args d_model (int) : Dimension of model. dropout (float) : Dropout after positional encoding. max_len : Max length of precalculated position sequence. forward def forward ( x )","title":"Utils"},{"location":"model/utils.html#get_dedup_tokens","text":"def get_dedup_tokens ( logits_batch ) Converts a batch of logits into the batch most probable tokens and their probabilities.","title":"get_dedup_tokens"},{"location":"model/utils.html#args","text":"logits_batch (Tensor) : torch.Tensor","title":"Args"},{"location":"model/utils.html#returns","text":"Tuple : Deduplicated tokens. The first element is a tensor (token indices) and the second element","title":"Returns"},{"location":"model/utils.html#_generate_square_subsequent_mask","text":"def _generate_square_subsequent_mask ( sz )","title":"_generate_square_subsequent_mask"},{"location":"model/utils.html#_make_len_mask","text":"def _make_len_mask ( inp )","title":"_make_len_mask"},{"location":"model/utils.html#_get_len_util_stop","text":"def _get_len_util_stop ( sequence , end_index )","title":"_get_len_util_stop"},{"location":"model/utils.html#_trim_util_stop","text":"def _trim_util_stop ( sequence , end_index )","title":"_trim_util_stop"},{"location":"model/utils.html#class-positionalencoding","text":"","title":"class PositionalEncoding"},{"location":"model/utils.html#__init__","text":"def __init__ ( d_model , dropout , max_len ) Initializes positional encoding.","title":"__init__"},{"location":"model/utils.html#args_1","text":"d_model (int) : Dimension of model. dropout (float) : Dropout after positional encoding. max_len : Max length of precalculated position sequence.","title":"Args"},{"location":"model/utils.html#forward","text":"def forward ( x )","title":"forward"},{"location":"preprocessing/text.html","text":"class LanguageTokenizer Simple tokenizer for language to index mapping. __init__ def __init__ ( languages ) Initializes a language tokenizer for a list of languages. Args languages (List[str]) : List of languages, e.g. ['de', 'en']. __call__ def __call__ ( lang ) Maps the language to an index. Args lang (str) : Language to be mapped, e.g. 'de'. Returns int : Index of language. decode def decode ( index ) Inverts the index mapping of a language. Args index (int) : Index of language. Returns str : Language for the given index. class SequenceTokenizer Tokenizes text and optionally attaches language-specific start index (and non-specific end index). __init__ def __init__ ( symbols , languages , char_repeats , lowercase , append_start_end , pad_token , end_token ) Initializes a SequenceTokenizer object. Args symbols (List[str]) : Character (or phoneme) symbols. languages (List[str]) : List of languages. char_repeats (int) : Number of repeats for each character to allow the forward model to map to longer phoneme sequences. Example lowercase (bool) : Whether to lowercase the input word. append_start_end (bool) : Whether to append special start and end tokens. Start and end tokens are index mappings of the chosen language. pad_token (str) : Special pad token for index 0. end_token (str) : Special end of sequence token. __call__ def __call__ ( sentence , language ) Maps a sequence of symbols for a language to a sequence of indices. Args sentence (Iterable[str]) : Sentence (or word) as a sequence of symbols. language (str) : Language for the mapping that defines the start and end token indices. Returns List[int] : Sequence of token indices. decode def decode ( sequence , remove_special_tokens ) Maps a sequence of indices to a sequence of symbols. Args sequence (Iterable[int]) : Encoded sequence to be decoded. remove_special_tokens (bool) : Whether to remove special tokens such as pad or start and end tokens. (Default value = False) sequence : Iterable[int] Returns List[str] : Decoded sequence of symbols. class Preprocessor Preprocesses data for a phonemizer training session. __init__ def __init__ ( lang_tokenizer , text_tokenizer , phoneme_tokenizer ) Initializes a preprocessor object. Args lang_tokenizer (LanguageTokenizer) : Tokenizer for input language. text_tokenizer (SequenceTokenizer) : Tokenizer for input text. phoneme_tokenizer (SequenceTokenizer) : Tokenizer for output phonemes. __call__ def __call__ ( item ) Preprocesses a data point. Args item (Tuple) : Data point comprised of (language, input text, output phonemes). from_config def from_config ( cls , config ) Initializes a preprocessor from a config. Args config (Dict[str, Any]) : Dictionary containing preprocessing hyperparams. Returns Preprocessor : Preprocessor object.","title":"Text"},{"location":"preprocessing/text.html#class-languagetokenizer","text":"Simple tokenizer for language to index mapping.","title":"class LanguageTokenizer"},{"location":"preprocessing/text.html#__init__","text":"def __init__ ( languages ) Initializes a language tokenizer for a list of languages.","title":"__init__"},{"location":"preprocessing/text.html#args","text":"languages (List[str]) : List of languages, e.g. ['de', 'en'].","title":"Args"},{"location":"preprocessing/text.html#__call__","text":"def __call__ ( lang ) Maps the language to an index.","title":"__call__"},{"location":"preprocessing/text.html#args_1","text":"lang (str) : Language to be mapped, e.g. 'de'.","title":"Args"},{"location":"preprocessing/text.html#returns","text":"int : Index of language.","title":"Returns"},{"location":"preprocessing/text.html#decode","text":"def decode ( index ) Inverts the index mapping of a language.","title":"decode"},{"location":"preprocessing/text.html#args_2","text":"index (int) : Index of language.","title":"Args"},{"location":"preprocessing/text.html#returns_1","text":"str : Language for the given index.","title":"Returns"},{"location":"preprocessing/text.html#class-sequencetokenizer","text":"Tokenizes text and optionally attaches language-specific start index (and non-specific end index).","title":"class SequenceTokenizer"},{"location":"preprocessing/text.html#__init___1","text":"def __init__ ( symbols , languages , char_repeats , lowercase , append_start_end , pad_token , end_token ) Initializes a SequenceTokenizer object.","title":"__init__"},{"location":"preprocessing/text.html#args_3","text":"symbols (List[str]) : Character (or phoneme) symbols. languages (List[str]) : List of languages. char_repeats (int) : Number of repeats for each character to allow the forward model to map to longer phoneme sequences. Example lowercase (bool) : Whether to lowercase the input word. append_start_end (bool) : Whether to append special start and end tokens. Start and end tokens are index mappings of the chosen language. pad_token (str) : Special pad token for index 0. end_token (str) : Special end of sequence token.","title":"Args"},{"location":"preprocessing/text.html#__call___1","text":"def __call__ ( sentence , language ) Maps a sequence of symbols for a language to a sequence of indices.","title":"__call__"},{"location":"preprocessing/text.html#args_4","text":"sentence (Iterable[str]) : Sentence (or word) as a sequence of symbols. language (str) : Language for the mapping that defines the start and end token indices.","title":"Args"},{"location":"preprocessing/text.html#returns_2","text":"List[int] : Sequence of token indices.","title":"Returns"},{"location":"preprocessing/text.html#decode_1","text":"def decode ( sequence , remove_special_tokens ) Maps a sequence of indices to a sequence of symbols.","title":"decode"},{"location":"preprocessing/text.html#args_5","text":"sequence (Iterable[int]) : Encoded sequence to be decoded. remove_special_tokens (bool) : Whether to remove special tokens such as pad or start and end tokens. (Default value = False) sequence : Iterable[int]","title":"Args"},{"location":"preprocessing/text.html#returns_3","text":"List[str] : Decoded sequence of symbols.","title":"Returns"},{"location":"preprocessing/text.html#class-preprocessor","text":"Preprocesses data for a phonemizer training session.","title":"class Preprocessor"},{"location":"preprocessing/text.html#__init___2","text":"def __init__ ( lang_tokenizer , text_tokenizer , phoneme_tokenizer ) Initializes a preprocessor object.","title":"__init__"},{"location":"preprocessing/text.html#args_6","text":"lang_tokenizer (LanguageTokenizer) : Tokenizer for input language. text_tokenizer (SequenceTokenizer) : Tokenizer for input text. phoneme_tokenizer (SequenceTokenizer) : Tokenizer for output phonemes.","title":"Args"},{"location":"preprocessing/text.html#__call___2","text":"def __call__ ( item ) Preprocesses a data point.","title":"__call__"},{"location":"preprocessing/text.html#args_7","text":"item (Tuple) : Data point comprised of (language, input text, output phonemes).","title":"Args"},{"location":"preprocessing/text.html#from_config","text":"def from_config ( cls , config ) Initializes a preprocessor from a config.","title":"from_config"},{"location":"preprocessing/text.html#args_8","text":"config (Dict[str, Any]) : Dictionary containing preprocessing hyperparams.","title":"Args"},{"location":"preprocessing/text.html#returns_4","text":"Preprocessor : Preprocessor object.","title":"Returns"},{"location":"preprocessing/utils.html","text":"_product def _product ( probs ) _batchify def _batchify ( input , batch_size )","title":"Utils"},{"location":"preprocessing/utils.html#_product","text":"def _product ( probs )","title":"_product"},{"location":"preprocessing/utils.html#_batchify","text":"def _batchify ( input , batch_size )","title":"_batchify"},{"location":"training/dataset.html","text":"collate_dataset def collate_dataset ( batch ) new_dataloader def new_dataloader ( dataset_file , batch_size , drop_last , use_binning ) class PhonemizerDataset __init__ def __init__ ( items ) __getitem__ def __getitem__ ( index ) __len__ def __len__ () class BinnedLengthSampler __init__ def __init__ ( phoneme_lens , batch_size , bin_size , seed ) __iter__ def __iter__ () __len__ def __len__ ()","title":"Dataset"},{"location":"training/dataset.html#collate_dataset","text":"def collate_dataset ( batch )","title":"collate_dataset"},{"location":"training/dataset.html#new_dataloader","text":"def new_dataloader ( dataset_file , batch_size , drop_last , use_binning )","title":"new_dataloader"},{"location":"training/dataset.html#class-phonemizerdataset","text":"","title":"class PhonemizerDataset"},{"location":"training/dataset.html#__init__","text":"def __init__ ( items )","title":"__init__"},{"location":"training/dataset.html#__getitem__","text":"def __getitem__ ( index )","title":"__getitem__"},{"location":"training/dataset.html#__len__","text":"def __len__ ()","title":"__len__"},{"location":"training/dataset.html#class-binnedlengthsampler","text":"","title":"class BinnedLengthSampler"},{"location":"training/dataset.html#__init___1","text":"def __init__ ( phoneme_lens , batch_size , bin_size , seed )","title":"__init__"},{"location":"training/dataset.html#__iter__","text":"def __iter__ ()","title":"__iter__"},{"location":"training/dataset.html#__len___1","text":"def __len__ ()","title":"__len__"},{"location":"training/decorators.html","text":"ignore_exception def ignore_exception ( f )","title":"Decorators"},{"location":"training/decorators.html#ignore_exception","text":"def ignore_exception ( f )","title":"ignore_exception"},{"location":"training/evaluation.html","text":"evaluate_samples def evaluate_samples ( lang_samples ) Calculates word and phoneme error rates per language and their mean across languages Args lang_samples (Dict) : Data to evaluate. Contains languages as keys and list of result samples as values. Prediction samples is given as a List of Tuples, where each Tuple is a tokenized representation of (text, result, target). Returns Dict : Evaluation result carrying word and phoneme error rates per language.","title":"Evaluation"},{"location":"training/evaluation.html#evaluate_samples","text":"def evaluate_samples ( lang_samples ) Calculates word and phoneme error rates per language and their mean across languages","title":"evaluate_samples"},{"location":"training/evaluation.html#args","text":"lang_samples (Dict) : Data to evaluate. Contains languages as keys and list of result samples as values. Prediction samples is given as a List of Tuples, where each Tuple is a tokenized representation of (text, result, target).","title":"Args"},{"location":"training/evaluation.html#returns","text":"Dict : Evaluation result carrying word and phoneme error rates per language.","title":"Returns"},{"location":"training/losses.html","text":"class CrossEntropyLoss __init__ def __init__ () forward def forward ( pred , batch ) Forward pass of the CrossEntropyLoss module on a batch. Args pred : torch.Tensor batch : Dict[str torch.Tensor] : Returns class CTCLoss __init__ def __init__ () forward def forward ( pred , batch ) Forward pass of the CTCLoss module on a batch. Args pred : torch.Tensor batch : Dict[str ext_len' : input text lengths, 'phonemes_len' torch.Tensor] : Returns","title":"Losses"},{"location":"training/losses.html#class-crossentropyloss","text":"","title":"class CrossEntropyLoss"},{"location":"training/losses.html#__init__","text":"def __init__ ()","title":"__init__"},{"location":"training/losses.html#forward","text":"def forward ( pred , batch ) Forward pass of the CrossEntropyLoss module on a batch.","title":"forward"},{"location":"training/losses.html#args","text":"pred : torch.Tensor batch : Dict[str torch.Tensor] :","title":"Args"},{"location":"training/losses.html#returns","text":"","title":"Returns"},{"location":"training/losses.html#class-ctcloss","text":"","title":"class CTCLoss"},{"location":"training/losses.html#__init___1","text":"def __init__ ()","title":"__init__"},{"location":"training/losses.html#forward_1","text":"def forward ( pred , batch ) Forward pass of the CTCLoss module on a batch.","title":"forward"},{"location":"training/losses.html#args_1","text":"pred : torch.Tensor batch : Dict[str ext_len' : input text lengths, 'phonemes_len' torch.Tensor] :","title":"Args"},{"location":"training/losses.html#returns_1","text":"","title":"Returns"},{"location":"training/metrics.html","text":"word_error def word_error ( predicted , target ) Calculates the word error rate of a single word result. Args predicted : List[Union[str target : List[Union[str int]] : Returns phoneme_error def phoneme_error ( predicted , target ) Calculates the phoneme error rate of a single result based on the Levenshtein distance. Args predicted : List[Union[str target : List[Union[str int]] : Returns","title":"Metrics"},{"location":"training/metrics.html#word_error","text":"def word_error ( predicted , target ) Calculates the word error rate of a single word result.","title":"word_error"},{"location":"training/metrics.html#args","text":"predicted : List[Union[str target : List[Union[str int]] :","title":"Args"},{"location":"training/metrics.html#returns","text":"","title":"Returns"},{"location":"training/metrics.html#phoneme_error","text":"def phoneme_error ( predicted , target ) Calculates the phoneme error rate of a single result based on the Levenshtein distance.","title":"phoneme_error"},{"location":"training/metrics.html#args_1","text":"predicted : List[Union[str target : List[Union[str int]] :","title":"Args"},{"location":"training/metrics.html#returns_1","text":"","title":"Returns"},{"location":"training/trainer.html","text":"class Trainer Performs model training. __init__ def __init__ ( checkpoint_dir , loss_type ) Initializes a Trainer object. Args checkpoint_dir (Path) : Directory to store the model checkpoints. loss_type (str) : Type of loss train def train ( model , checkpoint , store_phoneme_dict_in_model ) Performs training of a transformer model. Args model (Model) : Model to be trained (can be a fresh model or restored from a checkpoint). checkpoint (Dict[str, Any]) : Dictionary with entries 'optimizer' store_phoneme_dict_in_model (bool) : Whether to store a dictionary of word-phoneme mappings in the model checkpoint so that it can be automatically loaded by a Phonemizer object. Returns None : the checkpoints will be stored in a folder provided when instantiating a Trainer.","title":"Trainer"},{"location":"training/trainer.html#class-trainer","text":"Performs model training.","title":"class Trainer"},{"location":"training/trainer.html#__init__","text":"def __init__ ( checkpoint_dir , loss_type ) Initializes a Trainer object.","title":"__init__"},{"location":"training/trainer.html#args","text":"checkpoint_dir (Path) : Directory to store the model checkpoints. loss_type (str) : Type of loss","title":"Args"},{"location":"training/trainer.html#train","text":"def train ( model , checkpoint , store_phoneme_dict_in_model ) Performs training of a transformer model.","title":"train"},{"location":"training/trainer.html#args_1","text":"model (Model) : Model to be trained (can be a fresh model or restored from a checkpoint). checkpoint (Dict[str, Any]) : Dictionary with entries 'optimizer' store_phoneme_dict_in_model (bool) : Whether to store a dictionary of word-phoneme mappings in the model checkpoint so that it can be automatically loaded by a Phonemizer object.","title":"Args"},{"location":"training/trainer.html#returns","text":"None : the checkpoints will be stored in a folder provided when instantiating a Trainer.","title":"Returns"},{"location":"utils/io.html","text":"read_config def read_config ( path ) Reads the config dictionary from the yaml file. Args path (str) : Path to the .yaml file. Returns Dict[str, Any] : Configuration. save_config def save_config ( config , path ) Saves the config as a yaml file. Args config (Dict[str, Any]) : Configuration. path (str) : Path to save the dictionary to (.yaml). get_files def get_files ( path , extension ) Recursively retrieves all files with a given extension from a folder. Args path (str) : Path to the folder to retrieve files from. extension (str) : Extension of files to be retrieved (Default value = '.wav'). Returns List[Path] : List of paths to the found files. pickle_binary def pickle_binary ( data , file ) Pickles a given object to a binary file. Args data (object) : Object to be pickled. file (Union[str, Path]) : Path to destination file (use the .pkl extension). unpickle_binary def unpickle_binary ( file ) Unpickles a given binary file to an object Args file (nion[str, Path]) : Path to the file. Returns object : Unpickled object. to_device def to_device ( batch , device ) Sends a batch of data to the given torch devicee (cpu or cuda). Args batch (Dict[str, torch.Tensor]) : Batch to be send to the device. device (torch.device) : Device (either torch.device('cpu') or torch.device('cuda'). Returns Dict[str, torch.Tensor] : The batch at the given device.","title":"Io"},{"location":"utils/io.html#read_config","text":"def read_config ( path ) Reads the config dictionary from the yaml file.","title":"read_config"},{"location":"utils/io.html#args","text":"path (str) : Path to the .yaml file.","title":"Args"},{"location":"utils/io.html#returns","text":"Dict[str, Any] : Configuration.","title":"Returns"},{"location":"utils/io.html#save_config","text":"def save_config ( config , path ) Saves the config as a yaml file.","title":"save_config"},{"location":"utils/io.html#args_1","text":"config (Dict[str, Any]) : Configuration. path (str) : Path to save the dictionary to (.yaml).","title":"Args"},{"location":"utils/io.html#get_files","text":"def get_files ( path , extension ) Recursively retrieves all files with a given extension from a folder.","title":"get_files"},{"location":"utils/io.html#args_2","text":"path (str) : Path to the folder to retrieve files from. extension (str) : Extension of files to be retrieved (Default value = '.wav').","title":"Args"},{"location":"utils/io.html#returns_1","text":"List[Path] : List of paths to the found files.","title":"Returns"},{"location":"utils/io.html#pickle_binary","text":"def pickle_binary ( data , file ) Pickles a given object to a binary file.","title":"pickle_binary"},{"location":"utils/io.html#args_3","text":"data (object) : Object to be pickled. file (Union[str, Path]) : Path to destination file (use the .pkl extension).","title":"Args"},{"location":"utils/io.html#unpickle_binary","text":"def unpickle_binary ( file ) Unpickles a given binary file to an object","title":"unpickle_binary"},{"location":"utils/io.html#args_4","text":"file (nion[str, Path]) : Path to the file.","title":"Args"},{"location":"utils/io.html#returns_2","text":"object : Unpickled object.","title":"Returns"},{"location":"utils/io.html#to_device","text":"def to_device ( batch , device ) Sends a batch of data to the given torch devicee (cpu or cuda).","title":"to_device"},{"location":"utils/io.html#args_5","text":"batch (Dict[str, torch.Tensor]) : Batch to be send to the device. device (torch.device) : Device (either torch.device('cpu') or torch.device('cuda').","title":"Args"},{"location":"utils/io.html#returns_3","text":"Dict[str, torch.Tensor] : The batch at the given device.","title":"Returns"},{"location":"utils/logging.html","text":"get_logger def get_logger ( name ) Creates a logger object for a given name. Args name (str) : Name of the logger. Returns Logger : Logger object with given name.","title":"Logging"},{"location":"utils/logging.html#get_logger","text":"def get_logger ( name ) Creates a logger object for a given name.","title":"get_logger"},{"location":"utils/logging.html#args","text":"name (str) : Name of the logger.","title":"Args"},{"location":"utils/logging.html#returns","text":"Logger : Logger object with given name.","title":"Returns"}]}